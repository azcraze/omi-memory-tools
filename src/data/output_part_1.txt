Project Structure: da0d2e11-a414-4670-a291-e0baa99fccec └── memory-processing-toolbox ├── .DS_Store ├── .env ├── package.json └── src // File: memory-processing-toolbox/package.json //--------------------------------------------- { "name": "memory-processing-toolbox", "version": "1.0.0", "description": "A toolbox for processing and analyzing conversation memories.", "main": "src/scripts/index.js", "type": "commonjs", "scripts": { "setup": "node src/scripts/setupProject.js", "cli": "node src/scripts/index.js", "start-webhook": "node src/webhook/webhookServer.js", "test": "jest", "lint": "eslint .", "format": "prettier --write ." }, "keywords": [ "memory", "processing", "toolbox", "conversations", "analytics" ], "author": "Your Name <youremail@example.com>", "license": "MIT", "dependencies": { "ajv": "^8.11.0", "async-retry": "^1.3.3", "body-parser": "^1.19.0", "chalk": "^4.1.2", "dotenv": "^16.0.0", "express": "^4.17.1", "inquirer": "^8.2.4", "json2csv": "^5.0.7", "lodash": "^4.17.21", "ora": "^5.4.1" }, "devDependencies": { "eslint": "^8.23.0", "jest": "^29.0.0", "prettier": "^2.7.1" } } // File: memory-processing-toolbox/src/scripts/runExtractPlugins.js //----------------------------------------------------------------- // src/scripts/runExtractPlugins.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { filterNonDiscarded } = require('../modules/filterNonDiscarded.js'); const { extractPluginResponses } = require('../modules/extractPluginResponses.js'); const { writeJsonToCsv } = require('../utils/writeToCsv.js'); const { ValidationError } = require('../utils/customErrors.js'); /** * runExtractPlugins * Extracts plugin responses and writes them to JSON and CSV. */ async function runExtractPlugins() { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); const outputDirJson = path.join(process.cwd(), 'output', 'reports'); const outputDirCsv = path.join(process.cwd(), 'output', 'csv'); const pluginsJsonPath = path.join(outputDirJson, 'pluginResponses.json'); const pluginsCsvPath = path.join(outputDirCsv, 'pluginResponses.csv'); try { // Ensure output directories exist await Promise.all([ fs.mkdir(outputDirJson, { recursive: true }), fs.mkdir(outputDirCsv, { recursive: true }) ]); // Read conversations.json const rawData = await fs.readFile(conversationsPath, 'utf-8'); const allMemories = JSON.parse(rawData); // Filter out discarded/deleted memories const activeMemories = filterNonDiscarded(allMemories); // Extract plugin responses const pluginResponses = extractPluginResponses(activeMemories); // Write to JSON await fs.writeFile(pluginsJsonPath, JSON.stringify(pluginResponses, null, 2), 'utf-8'); logger.info(`Plugin responses written to JSON: ${pluginsJsonPath}`); // Convert plugin responses to CSV data const pluginCsvData = convertToCsvData(pluginResponses); // Write to CSV await writeJsonToCsv(pluginCsvData, pluginsCsvPath); logger.info(`Plugin responses written to CSV: ${pluginsCsvPath}`); } catch (error) { // Log and rethrow the error logger.error(`runExtractPlugins failed: ${error.message}`); throw error; } } /** * convertToCsvData * Transforms plugin responses into a flat array suitable for CSV conversion. * @param {Array} pluginResponses * @returns {Array} Flattened plugin data */ function convertToCsvData(pluginResponses) { const pluginFields = ['memoryId', 'pluginId', 'displayName', 'content']; return pluginResponses.flatMap(mem => mem.plugins.map(plugin => ({ memoryId: mem.memoryId, pluginId: plugin.pluginId, displayName: plugin.displayName, content: plugin.content })) ); } module.exports = { runExtractPlugins }; // File: memory-processing-toolbox/src/scripts/.DS_Store //------------------------------------------------------ // File: memory-processing-toolbox/src/scripts/runValidationCheck.js //------------------------------------------------------------------ // src/scripts/runValidationCheck.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { validateMemories } = require('../utils/verifyValidData.js'); const { ValidationError } = require('../utils/customErrors.js'); /** * runValidationCheck * Validates conversations.json against the defined schema. */ async function runValidationCheck() { try { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); // Read conversations.json let rawData; try { rawData = await fs.readFile(conversationsPath, 'utf-8'); } catch (readError) { logger.error(`Failed to read conversations.json: ${readError.message}`); throw new ValidationError('Failed to read conversations.json'); } let allMemories; try { allMemories = JSON.parse(rawData); } catch (parseError) { logger.error(`Failed to parse conversations.json: ${parseError.message}`); throw new ValidationError('Invalid JSON format in conversations.json'); } // Validate memories const { validMemories, invalidMemories } = validateMemories(allMemories); if (invalidMemories.length > 0) { logger.warn(`${invalidMemories.length} memories failed validation.`); // Optionally, write invalid memories to a separate file const invalidPath = path.join(process.cwd(), 'output', 'reports', 'invalidMemories.json'); await fs.mkdir(path.dirname(invalidPath), { recursive: true }); await fs.writeFile(invalidPath, JSON.stringify(invalidMemories, null, 2), 'utf-8'); logger.info(`Invalid memories written to: ${invalidPath}`); throw new ValidationError('Some memories failed validation. Check invalidMemories.json for details.'); } logger.info('All conversations are valid according to the schema.'); } catch (error) { logger.error(`runValidationCheck failed: ${error.message}`); throw error; // Rethrow to let the CLI handle it } } module.exports = { runValidationCheck, }; // File: memory-processing-toolbox/src/scripts/runWriteCsv.js //----------------------------------------------------------- // src/scripts/runWriteCsv.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { writeJsonToCsv } = require('../utils/writeToCsv.js'); const { filterNonDiscarded } = require('../modules/filterNonDiscarded.js'); const { extractTranscriptSegments } = require('../modules/extractTranscriptSegments.js'); const { extractCategories } = require('../modules/extractCategories.js'); const { extractPluginResponses } = require('../modules/extractPluginResponses.js'); const { ValidationError, MemoryProcessingError, FileOperationError } = require('../utils/customErrors.js'); /** * runWriteCsv * Converts specific data into CSV format and writes to files. */ async function runWriteCsv() { try { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); const rawData = await fs.readFile(conversationsPath, 'utf-8'); const allMemories = JSON.parse(rawData); // Filter out discarded/deleted const activeMemories = filterNonDiscarded(allMemories); // Extract data const categoriesSummary = extractCategories(activeMemories); const transcriptData = extractTranscriptSegments(activeMemories); const pluginResponses = extractPluginResponses(activeMemories); // Define fields for CSV const categoryFields = ['category', 'count', 'emoji']; const transcriptFields = ['memoryId', 'text', 'speaker', 'speaker_id', 'is_user', 'start', 'end']; const pluginFields = ['memoryId', 'pluginId', 'displayName', 'content']; // Flatten transcripts for CSV const flattenedTranscripts = transcriptData.flatMap(mem => mem.transcript.map(seg => ({ memoryId: mem.memoryId, text: seg.text, speaker: seg.speaker, speaker_id: seg.speaker_id, is_user: seg.is_user, start: seg.start, end: seg.end, })) ); // Flatten plugin responses for CSV const flattenedPlugins = pluginResponses.flatMap(mem => mem.plugins.map(plugin => ({ memoryId: mem.memoryId, pluginId: plugin.pluginId, displayName: plugin.displayName, content: plugin.content, })) ); // Ensure output directories exist const outputDir = path.join(process.cwd(), 'output', 'csv'); try { await fs.mkdir(outputDir, { recursive: true }); } catch (mkdirError) { logger.error(`Failed to create output directory: ${mkdirError.message }`); throw new FileOperationError('Directory creation failed'); } // Write Categories CSV const categoriesCsvPath = path.join(outputDir, 'categoriesSummary.csv'); try { await writeJsonToCsv(categoriesSummary, categoryFields, categoriesCsvPath); } catch (csvError) { logger.error(`Failed to write categories CSV: ${csvError.message }`); throw new ValidationError('Failed to write CSV file'); } // Write Transcripts CSV const transcriptsCsvPath = path.join(outputDir, 'transcripts.csv'); try { await writeJsonToCsv(flattenedTranscripts, transcriptFields, transcriptsCsvPath); } catch (csvError) { logger.error(`Failed to write transcripts CSV: ${csvError.message }`); throw new ValidationError('Failed to write CSV file'); } // Write Plugin Responses CSV const pluginsCsvPath = path.join(outputDir, 'pluginResponses.csv'); try { await writeJsonToCsv(flattenedPlugins, pluginFields, pluginsCsvPath); } catch (csvError) { logger.error(`Failed to write plugin responses CSV: ${csvError.message }`); throw new ValidationError('Failed to write CSV file'); } } catch (error) { logger.error(`runWriteCsv failed: ${error.message }`); throw error; // Rethrow to let the CLI handle it } } module.exports = { runWriteCsv, }; // File: memory-processing-toolbox/src/scripts/runFiltering.js //------------------------------------------------------------ // src/scripts/runFiltering.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { filterNonDiscarded } = require('../modules/filterNonDiscarded.js'); const { ValidationError } = require('../utils/customErrors.js'); /** * runFiltering * Filters out discarded/deleted memories from conversations.json. */ async function runFiltering() { try { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); // Read conversations.json let rawData; try { rawData = await fs.readFile(conversationsPath, 'utf-8'); } catch (readError) { logger.error(`Failed to read conversations.json: ${readError.message}`); throw new ValidationError('Failed to read conversations.json'); } let allMemories; try { allMemories = JSON.parse(rawData); } catch (parseError) { logger.error(`Failed to parse conversations.json: ${parseError.message}`); throw new ValidationError('Invalid JSON format in conversations.json'); } // Filter out discarded/deleted const filteredMemories = filterNonDiscarded(allMemories); // Write back to conversations.json try { await fs.writeFile(conversationsPath, JSON.stringify(filteredMemories, null, 2), 'utf-8'); logger.info(`Filtered conversations.json. Remaining memories: ${filteredMemories.length}`); } catch (writeError) { logger.error(`Failed to write to conversations.json: ${writeError.message}`); throw new ValidationError('Failed to write filtered memories'); } } catch (error) { logger.error(`runFiltering failed: ${error.message}`); throw error; // Rethrow to let the CLI handle it } } module.exports = { runFiltering, }; // File: memory-processing-toolbox/src/scripts/index.js //----------------------------------------------------- // src/scripts/index.js const inquirer = require('inquirer'); const chalk = require('chalk'); const ora = require('ora'); const logger = require('../utils/logger.js'); // Import the script functions (now async or Promise-based) const { runValidationCheck } = require('./runValidationCheck.js'); const { runFiltering } = require('./runFiltering.js'); const { runWriteCategories } = require('./runWriteCategories.js'); const { runExtractTranscripts } = require('./runExtractTranscripts.js'); const { runTextAnalysis } = require('./runTextAnalysis.js'); const { runExtractPlugins } = require('./runExtractPlugins.js'); const { runWriteMarkdown } = require('./runWriteMarkdown.js'); const { runWriteCsv } = require('./runWriteCsv.js'); /** * Helper to run a script function with an ora spinner. * * @param {Function} scriptFn - the function to run (async or Promise-based) * @param {string} message - the spinner text displayed while running * @returns {Promise<void>} */ async function runWithSpinner(scriptFn, message) { const spinner = ora({ text: message, color: 'cyan' }).start(); try { await scriptFn(); // run the actual script spinner.succeed(chalk.green(`${message} - Completed!`)); } catch (err) { spinner.fail(chalk.red(`${message} - Failed!`)); logger.error(err.message); // Optionally, display additional error information to the user console.error(chalk.red(`Error: ${err.message}`)); } } /** * runAllTasks * Demonstrates running all tasks in a specific sequence, * awaiting each one before moving to the next. */ async function runAllTasks() { logger.info(chalk.yellow('Running all tasks in sequence...')); try { await runWithSpinner(runValidationCheck, '1) Validating conversations'); await runWithSpinner(runFiltering, '2) Filtering memories'); await runWithSpinner(runWriteCategories, '3) Extracting categories'); await runWithSpinner(runExtractTranscripts, '4) Extracting transcripts'); await runWithSpinner(runTextAnalysis, '5) Performing text analysis'); await runWithSpinner(runExtractPlugins, '6) Extracting plugin responses'); await runWithSpinner(runWriteMarkdown, '7) Writing Markdown outputs'); await runWithSpinner(runWriteCsv, '8) Writing CSV outputs'); logger.info(chalk.magenta('All tasks completed successfully!')); } catch (error) { logger.error(`runAllTasks failed: ${error.message}`); console.error(chalk.red('One or more tasks failed. Check logs for details.')); } } /** * mainMenu * Displays a menu of available tasks using Inquirer, * then runs the chosen function (with spinners and color). */ async function mainMenu() { logger.info(chalk.bold.blue('Welcome to the memories-processing-toolbox CLI!')); const answers = await inquirer.prompt([ { type: 'list', name: 'task', message: chalk.yellow('Which task would you like to run?'), choices: [ { name: 'Validate Conversations (Schema Check)', value: 'validate' }, { name: 'Filter Discarded/Deleted Memories', value: 'filter' }, { name: 'Extract Categories', value: 'categories' }, { name: 'Extract Transcripts', value: 'transcripts' }, { name: 'Perform Text Analysis (word freq, keywords)', value: 'textAnalysis' }, { name: 'Extract Plugin Responses', value: 'plugins' }, { name: 'Write Markdown Outputs', value: 'markdown' }, { name: 'Write CSV Outputs', value: 'csv' }, new inquirer.Separator(), { name: 'Run ALL tasks in sequence', value: 'all' }, { name: 'Exit', value: 'exit' }, ], }, ]); switch (answers.task) { case 'validate': await runWithSpinner(runValidationCheck, 'Validating conversations'); break; case 'filter': await runWithSpinner(runFiltering, 'Filtering memories'); break; case 'categories': await runWithSpinner(runWriteCategories, 'Extracting categories'); break; case 'transcripts': await runWithSpinner(runExtractTranscripts, 'Extracting transcripts'); break; case 'textAnalysis': await runWithSpinner(runTextAnalysis, 'Performing text analysis'); break; case 'plugins': await runWithSpinner(runExtractPlugins, 'Extracting plugin responses'); break; case 'markdown': await runWithSpinner(runWriteMarkdown, 'Writing Markdown outputs'); break; case 'csv': await runWithSpinner(runWriteCsv, 'Writing CSV outputs'); break; case 'all': await runAllTasks(); break; case 'exit': logger.info(chalk.cyan('Exiting CLI...')); return; default: logger.warn('Unknown option selected. Exiting.'); return; } // Re-run the menu for continuous interaction await mainMenu(); } module.exports = { mainMenu, }; // File: memory-processing-toolbox/src/scripts/runExtractTranscripts.js //--------------------------------------------------------------------- // src/scripts/runExtractTranscripts.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { filterNonDiscarded } = require('../modules/filterNonDiscarded.js'); const { extractTranscriptSegments } = require('../modules/extractTranscriptSegments.js'); const { formatTranscriptMarkdown } = require('../utils/formatMarkdown.js'); const { writeJsonToCsv } = require('../utils/writeToCsv.js'); const { validateMemories } = require('../utils/verifyValidData.js'); const { ValidationError, MemoryProcessingError, FileOperationError } = require('../utils/customErrors.js'); /** * runExtractTranscripts * Extracts transcript segments and writes them to Markdown, JSON, and CSV. */ async function runExtractTranscripts() { try { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); // Read and parse conversations.json let rawData; try { rawData = await fs.readFile(conversationsPath, 'utf-8'); } catch (readError) { logger.error(`Failed to read conversations.json: ${readError.message }`); throw new FileOperationError('Failed to read conversations.json'); } let allMemories; try { allMemories = JSON.parse(rawData); } catch (parseError) { logger.error(`Failed to parse conversations.json: ${parseError.message }`); throw new ValidationError('Invalid JSON format in conversations.json'); } // Validate memories const { validMemories, invalidMemories } = validateMemories(allMemories); if (invalidMemories.length > 0) { logger.warn(`${invalidMemories.length} memories failed validation and will be skipped.`); // Optionally, log details or write to a separate file const invalidPath = path.join(process.cwd(), 'output', 'reports', 'invalidMemories.json'); await fs.mkdir(path.dirname(invalidPath), { recursive: true }); await fs.writeFile(invalidPath, JSON.stringify(invalidMemories, null, 2), 'utf-8'); logger.info(`Invalid memories written to: ${invalidPath}`); } // Proceed with valid memories const activeMemories = filterNonDiscarded(validMemories); // Extract transcripts let transcriptData; try { transcriptData = extractTranscriptSegments(activeMemories); } catch (extractError) { logger.error(`Failed to extract transcripts: ${extractError.message }`); throw new MemoryProcessingError('Transcript extraction failed'); } // Format to Markdown let transcriptsMarkdown; try { transcriptsMarkdown = formatTranscriptMarkdown(transcriptData); } catch (formatError) { logger.error(`Failed to format transcripts to Markdown: ${formatError.message }`); throw new MemoryProcessingError('Markdown formatting failed'); } // Ensure output directories exist const outputDirMd = path.join(process.cwd(), 'output', 'markdown'); const outputDirJson = path.join(process.cwd(), 'output', 'reports'); const outputDirCsv = path.join(process.cwd(), 'output', 'csv'); try { await fs.mkdir(outputDirMd, { recursive: true }); await fs.mkdir(outputDirJson, { recursive: true }); await fs.mkdir(outputDirCsv, { recursive: true }); } catch (mkdirError) { logger.error(`Failed to create output directories: ${mkdirError.message }`); throw new FileOperationError('Directory creation failed'); } // Write to Markdown const transcriptsMdPath = path.join(outputDirMd, 'transcripts.md'); try { await fs.writeFile(transcriptsMdPath, transcriptsMarkdown, 'utf-8'); logger.info(`Transcripts written to Markdown: ${transcriptsMdPath}`); } catch (writeError) { logger.error(`Failed to write transcripts Markdown: ${writeError.message }`); throw new FileOperationError('Failed to write Markdown file'); } // Write to JSON const transcriptsJsonPath = path.join(outputDirJson, 'transcripts.json'); try { await fs.writeFile(transcriptsJsonPath, JSON.stringify(transcriptData, null, 2), 'utf-8'); logger.info(`Transcripts written to JSON: ${transcriptsJsonPath}`); } catch (writeError) { logger.error(`Failed to write transcripts JSON: ${writeError.message }`); throw new FileOperationError('Failed to write JSON file'); } // Write to CSV const transcriptFields = ['memoryId', 'text', 'speaker', 'speaker_id', 'is_user', 'start', 'end']; const flattenedTranscripts = transcriptData.flatMap(mem => mem.transcript.map(seg => ({ memoryId: mem.memoryId, text: seg.text, speaker: seg.speaker, speaker_id: seg.speaker_id, is_user: seg.is_user, start: seg.start, end: seg.end, })) ); const transcriptsCsvPath = path.join(outputDirCsv, 'transcripts.csv'); try { await writeJsonToCsv(flattenedTranscripts, transcriptFields, transcriptsCsvPath); } catch (csvError) { logger.error(`Failed to write transcripts CSV: ${csvError.message }`); throw new ValidationError('Failed to write CSV file'); } } catch (error) { logger.error(`runExtractTranscripts failed: ${error.message }`); throw error; // Rethrow to let the CLI handle it } } module.exports = { runExtractTranscripts, }; // File: memory-processing-toolbox/src/scripts/runTextAnalysis.js //--------------------------------------------------------------- // src/scripts/runTextAnalysis.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { getWordFrequency, getKeywords } = require('../utils/textAnalysis.js'); const { validateMemories } = require('../utils/verifyValidData.js'); const { filterNonDiscarded } = require('../modules/filterNonDiscarded.js'); const { writeJsonToCsv } = require('../utils/writeToCsv.js'); const { ValidationError, MemoryProcessingError, FileOperationError } = require('../utils/customErrors.js'); /** * runTextAnalysis * Performs text analysis on transcripts, including word frequency and keyword extraction. */ async function runTextAnalysis() { try { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); // Read and parse conversations.json let rawData; try { rawData = await fs.readFile(conversationsPath, 'utf-8'); } catch (readError) { logger.error(`Failed to read conversations.json: ${readError.message }`); throw new FileOperationError('Failed to read conversations.json'); } let allMemories; try { allMemories = JSON.parse(rawData); } catch (parseError) { logger.error(`Failed to parse conversations.json: ${parseError.message }`);;; throw new ValidationError('Invalid JSON format in conversations.json'); } // Validate memories const { validMemories, invalidMemories } = validateMemories(allMemories); if (invalidMemories.length > 0) { logger.warn(`${invalidMemories.length} memories failed validation and will be skipped.`); // Optionally, log details or write to a separate file const invalidPath = path.join(process.cwd(), 'output', 'reports', 'invalidMemories.json'); await fs.mkdir(path.dirname(invalidPath), { recursive: true }); await fs.writeFile(invalidPath, JSON.stringify(invalidMemories, null, 2), 'utf-8'); logger.info(`Invalid memories written to: ${invalidPath}`); } // Proceed with valid memories const activeMemories = filterNonDiscarded(validMemories); // Aggregate all transcript segments const allTranscripts = activeMemories.flatMap(mem => mem.transcript_segments || []); // Calculate word frequency const wordFreq = getWordFrequency(allTranscripts, { n: 1, stopwords: undefined, groupBySpeaker: false }); // Extract top 10 keywords const topKeywords = getKeywords(wordFreq, 10); // Format results const textAnalysisResults = { wordFrequency: wordFreq, topKeywords, }; // Ensure output directories exist const outputDirJson = path.join(process.cwd(), 'output', 'reports'); const outputDirCsv = path.join(process.cwd(), 'output', 'csv'); try { await fs.mkdir(outputDirJson, { recursive: true }); await fs.mkdir(outputDirCsv, { recursive: true }); } catch (mkdirError) { logger.error(`Failed to create output directories: ${mkdirError.message }`); throw new FileOperationError('Directory creation failed'); } // Write to JSON const analysisJsonPath = path.join(outputDirJson, 'textAnalysis.json'); try { await fs.writeFile(analysisJsonPath, JSON.stringify(textAnalysisResults, null, 2), 'utf-8'); logger.info(`Text analysis results written to JSON: ${analysisJsonPath}`); } catch (writeError) { logger.error(`Failed to write text analysis JSON: ${writeError.message }`); throw new FileOperationError('Failed to write JSON file'); } // Write to CSV (word frequency) const wordFreqFields = ['word', 'count']; const wordFreqData = Object.entries(wordFreq).map(([word, count]) => ({ word, count })); const wordFreqCsvPath = path.join(outputDirCsv, 'wordFrequency.csv'); try { await writeJsonToCsv(wordFreqData, wordFreqFields, wordFreqCsvPath); } catch (csvError) { logger.error(`Failed to write word frequency CSV: ${csvError.message }`); throw new ValidationError('Failed to write CSV file'); } // Write to CSV (top keywords) const topKeywordsFields = ['keyword', 'count']; const topKeywordsCsvPath = path.join(outputDirCsv, 'topKeywords.csv'); try { await writeJsonToCsv(topKeywords, topKeywordsFields, topKeywordsCsvPath); } catch (csvError) { logger.error(`Failed to write top keywords CSV: ${csvError.message }`); throw new ValidationError('Failed to write CSV file'); } } catch (error) { logger.error(`runTextAnalysis failed: ${error.message }`); throw error; // Rethrow to let the CLI handle it } } module.exports = { runTextAnalysis, }; // File: memory-processing-toolbox/src/scripts/runWriteCategories.js //------------------------------------------------------------------ // src/scripts/runWriteCategories.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { filterNonDiscarded } = require('../modules/filterNonDiscarded.js'); const { extractCategories } = require('../modules/extractCategories.js'); const { formatCategoriesMarkdown } = require('../utils/formatMarkdown.js'); const { writeJsonToCsv } = require('../utils/writeToCsv.js'); const { ValidationError } = require('../utils/customErrors.js'); /** * runWriteCategories * Extracts category summaries and writes them to Markdown, JSON, and CSV. */ async function runWriteCategories() { try { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); // Read and parse conversations.json let rawData; try { rawData = await fs.readFile(conversationsPath, 'utf-8'); } catch (readError) { logger.error(`Failed to read conversations.json: ${readError.message}`); throw new ValidationError('Failed to read conversations.json'); } let allMemories; try { allMemories = JSON.parse(rawData); } catch (parseError) { logger.error(`Failed to parse conversations.json: ${parseError.message}`); throw new ValidationError('Invalid JSON format in conversations.json'); } // Filter out discarded/deleted const activeMemories = filterNonDiscarded(allMemories); // Extract categories let categoriesSummary; try { categoriesSummary = extractCategories(activeMemories); } catch (extractError) { logger.error(`Failed to extract categories: ${extractError.message}`); throw new MemoryProcessingError('Category extraction failed'); } // Format to Markdown let categoriesMarkdown; try { categoriesMarkdown = formatCategoriesMarkdown(categoriesSummary); } catch (formatError) { logger.error(`Failed to format categories to Markdown: ${formatError.message}`); throw new MemoryProcessingError('Markdown formatting failed'); } // Ensure output directories exist const outputDirMd = path.join(process.cwd(), 'output', 'markdown'); const outputDirJson = path.join(process.cwd(), 'output', 'reports'); const outputDirCsv = path.join(process.cwd(), 'output', 'csv'); try { await fs.mkdir(outputDirMd, { recursive: true }); await fs.mkdir(outputDirJson, { recursive: true }); await fs.mkdir(outputDirCsv, { recursive: true }); } catch (mkdirError) { logger.error(`Failed to create output directories: ${mkdirError.message }`); throw new ValidationError('Directory creation failed'); } // Write to Markdown const categoriesMdPath = path.join(outputDirMd, 'categoriesSummary.md'); try { await fs.writeFile(categoriesMdPath, categoriesMarkdown, 'utf-8'); logger.info(`Categories summary written to Markdown: ${categoriesMdPath}`); } catch (writeError) { logger.error(`Failed to write categories Markdown: ${writeError.message}`); throw new ValidationError('Failed to write Markdown file'); } // Write to JSON const categoriesJsonPath = path.join(outputDirJson, 'categoriesSummary.json'); try { await fs.writeFile(categoriesJsonPath, JSON.stringify(categoriesSummary, null, 2), 'utf-8'); logger.info(`Categories summary written to JSON: ${categoriesJsonPath}`); } catch (writeError) { logger.error(`Failed to write categories JSON: ${writeError.message}`); throw new ValidationError('Failed to write JSON file'); } // Write to CSV const categoryFields = ['category', 'count', 'emoji']; const categoriesCsvPath = path.join(outputDirCsv, 'categoriesSummary.csv'); try { await writeJsonToCsv(categoriesSummary, categoryFields, categoriesCsvPath); } catch (csvError) { logger.error(`Failed to write categories CSV: ${csvError.message }`); throw new ValidationError('Failed to write CSV file'); } } catch (error) { logger.error(`runWriteCategories failed: ${error.message }`); throw error; // Rethrow to let the CLI handle it } } module.exports = { runWriteCategories, }; // File: memory-processing-toolbox/src/scripts/runWriteMarkdown.js //---------------------------------------------------------------- // src/scripts/runWriteMarkdown.js const fs = require('fs').promises; const path = require('path'); const logger = require('../utils/logger.js'); const { formatCategoriesMarkdown, formatTranscriptMarkdown } = require('../utils/formatMarkdown.js'); const { filterNonDiscarded } = require('../modules/filterNonDiscarded.js'); const { extractCategories } = require('../modules/extractCategories.js'); const { extractTranscriptSegments } = require('../modules/extractTranscriptSegments.js'); const { extractPluginResponses } = require('../modules/extractPluginResponses.js'); const { runTextAnalysis } = require('./runTextAnalysis.js'); const { ValidationError, MemoryProcessingError, FileOperationError } = require('../utils/customErrors.js'); /** * runWriteMarkdown * Consolidates various extracted data into Markdown reports. */ async function runWriteMarkdown() { try { const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); // Read conversations.json let rawData; try { rawData = await fs.readFile(conversationsPath, 'utf-8'); } catch (readError) { logger.error(`Failed to read conversations.json: ${readError.message }`); throw new FileOperationError('Failed to read conversations.json'); } let allMemories; try { allMemories = JSON.parse(rawData); } catch (parseError) { logger.error(`Failed to parse conversations.json: ${parseError.message }`); throw new ValidationError('Invalid JSON format in conversations.json'); } // Filter out discarded/deleted const activeMemories = filterNonDiscarded(allMemories); // Extract categories let categoriesSummary; try { categoriesSummary = extractCategories(activeMemories); } catch (extractError) { logger.error(`Failed to extract categories: ${extractError.message }`); throw new MemoryProcessingError('Category extraction failed'); } // Extract transcripts let transcriptData; try { transcriptData = extractTranscriptSegments(activeMemories); } catch (extractError) { logger.error(`Failed to extract transcripts: ${extractError.message }`); throw new MemoryProcessingError('Transcript extraction failed'); } // Extract plugin responses let pluginResponses; try { pluginResponses = extractPluginResponses(activeMemories); } catch (extractError) { logger.error(`Failed to extract plugin responses: ${extractError.message }`); throw new MemoryProcessingError('Plugin response extraction failed'); } // Perform text analysis (optional, can be skipped if already done) // await runTextAnalysis(); // Format all data into Markdown let markdownContent = '# Memories Processing Toolbox Reports'; // Categories Summary markdownContent += formatCategoriesMarkdown(categoriesSummary); // Transcripts markdownContent += formatTranscriptMarkdown(transcriptData); // Plugin Responses markdownContent += '## Plugin Responses' pluginResponses.forEach(mem => { markdownContent += `### Memory ID: ${mem.memoryId} `; mem.plugins.forEach(plugin => { markdownContent += `**${plugin.displayName}**: ${plugin.content} `; }); }); // Add more sections as needed // Ensure output directory exists const outputDirMd = path.join(process.cwd(), 'output', 'markdown'); try { await fs.mkdir(outputDirMd, { recursive: true }); } catch (mkdirError) { logger.error(`Failed to create output directory: ${mkdirError.message }`); throw new FileOperationError('Directory creation failed'); } // Write to Markdown file const markdownPath = path.join(outputDirMd, 'consolidatedReport.md'); try { await fs.writeFile(markdownPath, markdownContent, 'utf-8'); logger.info(`Consolidated Markdown report written to: ${markdownPath}`); } catch (writeError) { logger.error(`Failed to write Markdown report: ${writeError.message }`); throw new FileOperationError('Failed to write Markdown file'); } } catch (error) { logger.error(`runWriteMarkdown failed: ${error.message }`); throw error; // Rethrow to let the CLI handle it } } module.exports = { runWriteMarkdown, }; // File: memory-processing-toolbox/src/webhook/webhookServer.js //------------------------------------------------------------- // src/webhook/webhookServer.js require('dotenv').config(); // Ensure environment variables are loaded const express = require('express'); const bodyParser = require('body-parser'); const crypto = require('crypto'); const logger = require('../utils/logger.js'); const fs = require('fs').promises; const path = require('path'); const retry = require('async-retry'); // Import your processing scripts const { runValidationCheck } = require('../scripts/runValidationCheck.js'); const { runFiltering } = require('../scripts/runFiltering.js'); const { runWriteCategories } = require('../scripts/runWriteCategories.js'); const { runExtractTranscripts } = require('../scripts/runExtractTranscripts.js'); const { runTextAnalysis } = require('../scripts/runTextAnalysis.js'); const { runExtractPlugins } = require('../scripts/runExtractPlugins.js'); const { runWriteMarkdown } = require('../scripts/runWriteMarkdown.js'); const { runWriteCsv } = require('../scripts/runWriteCsv.js'); const { MemoryProcessingError, FileOperationError, ValidationError, } = require('../utils/customErrors.js'); // Initialize Express App const app = express(); // Webhook Secret from environment variables const WEBHOOK_SECRET = process.env.WEBHOOK_SECRET; if (!WEBHOOK_SECRET) { logger.error('WEBHOOK_SECRET is not defined in environment variables.'); process.exit(1); // Exit the application if secret is missing } // Middleware to verify signature function verifySignature(req, res, buf, encoding) { const signature = req.headers['x-webhook-signature']; if (!signature) { throw new MemoryProcessingError('No signature found in headers'); } const hash = crypto.createHmac('sha256', WEBHOOK_SECRET) .update(buf) .digest('hex'); if (hash !== signature) { throw new MemoryProcessingError('Invalid signature'); } } // Use raw body parser with verification app.use(bodyParser.json({ verify: verifySignature })); // Error-handling middleware for body-parser app.use((err, req, res, next) => { if (err) { if (err instanceof MemoryProcessingError) { logger.error(`Webhook signature verification failed: ${err.message }`); return res.status(401).send('Unauthorized'); } logger.error(`Error parsing request body: ${err.message }`); return res.status(400).send('Invalid request body'); } next(); }); // Define the webhook endpoint app.post('/webhook', async (req, res) => { const event = req.body; logger.info('Received webhook event:', JSON.stringify(event, null, 2)); try { // Validate event structure (implement as needed) await handleNewConversationEvent(event); // Trigger processing scripts await runProcessingTasks(); // Respond to acknowledge receipt res.status(200).send('Event received and processed'); } catch (error) { logger.error('Error processing webhook event:', error.message); res.status(500).send('Internal Server Error'); } }); /** * Function to handle new conversation events with retries * @param {Object} event - The webhook event payload */ async function handleNewConversationEvent(event) { const memory = event.memory; if (!memory) { throw new ValidationError('Invalid event data: Missing memory object'); } const conversationsPath = path.join(process.cwd(), 'src', 'data', 'conversations.json'); // Retryable file operations await retry(async (bail) => { // Read existing conversations let conversations = []; try { // Check if file exists await fs.access(conversationsPath); const rawData = await fs.readFile(conversationsPath, 'utf-8'); conversations = JSON.parse(rawData); } catch (fileError) { if (fileError.code === 'ENOENT') { // File does not exist, start with empty array conversations = []; logger.warn('conversations.json does not exist. Starting with an empty array.'); } else { logger.error(`Failed to read conversations.json: ${fileError.message }`); throw new FileOperationError('Failed to read conversations.json'); } } // Add the new memory conversations.push(memory); // Write back to conversations.json try { await fs.writeFile(conversationsPath, JSON.stringify(conversations, null, 2), 'utf-8'); logger.info(`New memory added with ID: ${memory.id}`); } catch (writeError) { logger.error(`Failed to write to conversations.json: ${writeError.message }`); throw new FileOperationError('Failed to write new memory'); } }, { retries: 3, minTimeout: 1000, onRetry: (error, attempt) => { logger.warn(`Retrying due to error: ${error.message}. Attempt ${attempt}`); } }); } /** * Function to run all processing scripts sequentially */ async function runProcessingTasks() { try { await runValidationCheck(); await runFiltering(); await runWriteCategories(); await runExtractTranscripts(); await runTextAnalysis(); await runExtractPlugins(); await runWriteMarkdown(); await runWriteCsv(); logger.info('All processing tasks completed successfully.'); } catch (taskError) { logger.error(`Processing tasks failed: ${taskError.message}`); throw new MemoryProcessingError('Processing tasks failed'); } } /** * Function to handle graceful shutdown * @param {string} signal - The signal received */ function gracefulShutdown(signal) { logger.info(`Received ${signal}. Shutting down gracefully...`); server.close(() => { logger.info('Closed out remaining connections.'); process.exit(0); }); // Force shutdown after a timeout setTimeout(() => { logger.error('Could not close connections in time, forcefully shutting down'); process.exit(1); }, 10000); // 10 seconds } // Listen for termination signals process.on('SIGTERM', () => gracefulShutdown('SIGTERM')); process.on('SIGINT', () => gracefulShutdown('SIGINT')); // Global error handlers to catch unhandled promise rejections and exceptions process.on('unhandledRejection', (reason, promise) => { logger.error('Unhandled Rejection at:', promise, 'reason:', reason); // Optionally, perform cleanup or alerting }); process.on('uncaughtException', (error) => { logger.error('Uncaught Exception thrown:', error); // Optionally, perform cleanup or alerting process.exit(1); // Exit the process to avoid unknown state }); // Start the server const PORT = process.env.WEBHOOK_PORT || 3000; const server = app.listen(PORT, () => { logger.info(`Webhook server is listening on port ${PORT}`); }); // File: memory-processing-toolbox/src/data/conversations.json //------------------------------------------------------------ [ { "id": "be32019e-36c9-4b5f-8043-092cc2aaf5ce", "created_at": "2025-01-01T10:00:00Z", "structured": { "title": "Sample Conversation", "overview": "This is a sample conversation.", "emoji": "💬", "category": "Technology", "actionItems": ["Review project setup", "Implement webhook"], "events": [ { "title": "Kickoff Meeting", "starts_at": "2025-01-01T09:00:00Z", "duration": 60, "description": "Initial project kickoff.", "created": true } ] }, "started_at": "2025-01-01T09:00:00Z", "finished_at": "2025-01-01T10:00:00Z", "transcript_segments": [ { "text": "Hello, how are you?", "speaker": "SPEAKER_1", "speaker_id": 1, "is_user": true, "start": 0, "end": 5 }, { "text": "I'm good, thank you!", "speaker": "SPEAKER_2", "speaker_id": 2, "is_user": false, "start": 6, "end": 10 } ], "plugins_results": [ { "pluginId": "plugin123", "content": "Plugin content here." } ], "geolocation": null, "photos": [], "discarded": false, "deleted": false, "source": "ConversationSource.friend", "language": "en", "external_data": null, "status": "active" } ] // File: memory-processing-toolbox/src/data/memoriesSchema.json //------------------------------------------------------------- { "$schema": "http://json-schema.org/draft-07/schema#", "title": "Memories Schema", "type": "array", "items": { "type": "object", "properties": { "id": { "type": "string", "format": "uuid", "description": "Unique identifier for the conversation memory." }, "created_at": { "type": "string", "format": "date-time", "description": "Timestamp when the memory was created." }, "structured": { "type": "object", "properties": { "title": { "type": "string", "description": "Title of the conversation." }, "overview": { "type": "string", "description": "Overview or summary of the conversation." }, "emoji": { "type": "string", "description": "Emoji representing the conversation topic." }, "category": { "type": "string", "description": "Category of the conversation (e.g., technology)." }, "actionItems": { "type": "array", "items": { "type": "string" }, "description": "List of action items derived from the conversation." }, "events": { "type": "array", "items": { "type": "object", "properties": { "title": { "type": "string", "description": "Title of the event." }, "starts_at": { "type": "string", "format": "date-time", "description": "Start time of the event." }, "duration": { "type": "integer", "description": "Duration of the event in minutes." }, "description": { "type": "string", "description": "Detailed description of the event." }, "created": { "type": "boolean", "description": "Flag indicating if the event was created." } }, "required": ["title", "starts_at", "duration", "description", "created"], "additionalProperties": false } } }, "required": [ "title", "overview", "emoji", "category", "actionItems", "events" ], "additionalProperties": false }, "started_at": { "type": "string", "format": "date-time", "description": "Timestamp when the conversation started." }, "finished_at": { "type": "string", "format": "date-time", "description": "Timestamp when the conversation ended." }, "transcript_segments": { "type": "array", "items": { "type": "object", "properties": { "text": { "type": "string", "description": "Text of the transcript segment." }, "speaker": { "type": "string", "description": "Identifier for the speaker (e.g., SPEAKER_1)." }, "speaker_id": { "type": "integer", "description": "Numeric ID of the speaker." }, "is_user": { "type": "boolean", "description": "Flag indicating if the speaker is a user." }, "start": { "type": "number", "description": "Start time of the segment in seconds." }, "end": { "type": "number", "description": "End time of the segment in seconds." } }, "required": ["text", "speaker", "speaker_id", "is_user", "start", "end"], "additionalProperties": false }, "description": "Array of transcript segments from the conversation." }, "plugins_results": { "type": "array", "items": { "type": "object", "properties": { "pluginId": { "type": "string", "description": "Identifier of the plugin used." }, "content": { "type": "string", "description": "Content or result produced by the plugin." } }, "required": ["pluginId", "content"], "additionalProperties": true }, "description": "Results from various plugins associated with the conversation." }, "geolocation": { "oneOf": [ { "type": "object", "properties": { "latitude": { "type": "number", "description": "Latitude coordinate." }, "longitude": { "type": "number", "description": "Longitude coordinate." }, "altitude": { "type": "number", "description": "Altitude information." }, "accuracy": { "type": "number", "description": "Accuracy of the geolocation data." } }, "required": ["latitude", "longitude"], "additionalProperties": false }, { "type": "null", "description": "No geolocation data available." } ], "description": "Geolocation data associated with the conversation, if any." }, "photos": { "type": "array", "items": { "type": "object", "properties": { "url": { "type": "string", "format": "uri", "description": "URL of the photo." }, "description": { "type": "string", "description": "Description or metadata of the photo." }, "timestamp": { "type": "string", "format": "date-time", "description": "Timestamp when the photo was taken." } }, "required": ["url"], "additionalProperties": false }, "description": "Array of photos associated with the conversation." }, "discarded": { "type": "boolean", "description": "Flag indicating if the memory has been discarded." }, "deleted": { "type": "boolean", "description": "Flag indicating if the memory has been deleted." }, "source": { "type": "string", "enum": ["ConversationSource.friend"], "description": "Source of the conversation." }, "language": { "oneOf": [ { "type": "string", "description": "Language code of the conversation (e.g., en for English)." }, { "type": "null", "description": "No language data available." } ] }, "external_data": { "oneOf": [ { "type": "object", "description": "External data associated with the conversation." }, { "type": "null", "description": "No external data available." } ] }, "status": { "type": "string", "description": "Current status of the conversation memory." } }, "required": [ "id", "created_at", "structured", "started_at", "finished_at", "transcript_segments", "plugins_results", "discarded", "deleted", "source", "language", "status" ], "additionalProperties": true }, "description": "Array of conversation memory objects." } // File: memory-processing-toolbox/src/config.js //---------------------------------------------- // src/config.js const path = require('path'); const ROOT_DIR = process.cwd(); const LOG_DIR = path.join(ROOT_DIR, 'logs'); const OUTPUT_DIR = path.join(ROOT_DIR, 'output'); module.exports = { ROOT_DIR, LOG_DIR, OUTPUT_DIR, // Add more config variables here as needed }; // File: memory-processing-toolbox/src/utils/customErrors.js //---------------------------------------------------------- // src/utils/customErrors.js class MemoryProcessingError extends Error { constructor(message) { super(message); this.name = 'MemoryProcessingError'; } } class FileOperationError extends MemoryProcessingError { constructor(message) { super(message); this.name = 'FileOperationError'; } } class ValidationError extends MemoryProcessingError { constructor(message) { super(message); this.name = 'ValidationError'; } } class WebhookError extends MemoryProcessingError { constructor(message) { super(message); this.name = 'WebhookError'; } } // Add more custom error classes as needed module.exports = { MemoryProcessingError, FileOperationError, ValidationError, WebhookError, }; // File: memory-processing-toolbox/src/utils/textAnalysis.js //---------------------------------------------------------- // src/utils/textAnalysis.js const logger = require('../utils/logger.js'); const _ = require('lodash'); // A simple default stopwords list (English example). // You can expand this or move it to globalVariables.js if you prefer. const defaultStopwords = [ 'the', 'and', 'of', 'to', 'a', 'i', 'it', 'in', 'is', 'you', 'that', 'for', 'on', 'was', 'with', 'as', 'this',