'but', 'be', 'are', 'have', 'not', 'at', 'or', 'he', 'by', 'from' ]; /** * cleanAndTokenize * - Lowercases text, removes punctuation (basic approach), splits into tokens. * @param {string} text * @returns {Array<string>} tokens */ function cleanAndTokenize(text) { // Convert to lowercase let cleaned = text.toLowerCase(); // Remove punctuation (basic regex approach) // This will remove everything that isn't a letter, number, or space. cleaned = cleaned.replace(/[^a-z0-9s]+/gi, ''); // Split on whitespace const tokens = cleaned.split(/s+/).filter(Boolean); return tokens; } /** * getNgrams * - Converts an array of tokens into an array of n-grams. * e.g., tokens = ["i", "love", "red", "apples"], n=2 => ["i love", "love red", "red apples"] * @param {Array<string>} tokens * @param {number} n - e.g., 2 for bigrams, 3 for trigrams * @returns {Array<string>} */ function getNgrams(tokens, n) { const ngrams = []; for (let i = 0; i <= tokens.length - n; i++) { ngrams.push(tokens.slice(i, i + n).join(' ')); } return ngrams; } /** * getWordFrequency * - Calculates the frequency of each token or n-gram in the transcripts. * @param {Array<object>} transcripts - Array of { text, speaker, ... } * @param {object} options * - n (number): if > 1, calculates n-gram frequencies instead of single-word * - stopwords (Array<string>): custom array of words to ignore * - groupBySpeaker (boolean): if true, returns frequency per speaker * @returns {object} If groupBySpeaker=false => { "word": count, "word2": count, ... } * If groupBySpeaker=true => { speakerA: { "word": count, ... }, speakerB: {...}, ... } */ function getWordFrequency(transcripts = [], options = {}) { logger.info('Calculating word/n-gram frequency...'); const { n = 1, stopwords = defaultStopwords, groupBySpeaker = false, } = options; if (!groupBySpeaker) { // Single frequency map for the entire set of transcripts const freqMap = {}; transcripts.forEach(seg => { const tokens = cleanAndTokenize(seg.text); const finalTokens = n > 1 ? getNgrams(tokens, n) : tokens; finalTokens.forEach(token => { // Skip if token is a stopword (for single words only) // If n>1, tokens are phrases like "machine learning" if (n === 1 && stopwords.includes(token)) return; freqMap[token] = (freqMap[token] || 0) + 1; }); }); return freqMap; } else { // Frequency map separated by speaker const freqMapBySpeaker = {}; transcripts.forEach(seg => { const { speaker } = seg; if (!freqMapBySpeaker[speaker]) { freqMapBySpeaker[speaker] = {}; } const tokens = cleanAndTokenize(seg.text); const finalTokens = n > 1 ? getNgrams(tokens, n) : tokens; finalTokens.forEach(token => { if (n === 1 && stopwords.includes(token)) return; freqMapBySpeaker[speaker][token] = (freqMapBySpeaker[speaker][token] || 0) + 1; }); }); return freqMapBySpeaker; } } /** * getKeywords * - Returns the top N keywords/phrases across transcripts, ignoring stopwords. * This is essentially a "most frequent words" approach, filtered to exclude stopwords. * @param {object} freqMap - output from getWordFrequency (if groupBySpeaker=false) * @param {number} limit - how many top keywords to return * @returns {Array<object>} e.g. [ { keyword: "apple", count: 10 }, ... ] */ function getKeywords(freqMap, limit = 10) { logger.info(`Extracting top ${limit} keywords...`); // freqMap is { "word": count, "anotherWord": count, ... } const entries = Object.entries(freqMap); // Sort by count descending entries.sort((a, b) => b[1] - a[1]); // Return the top limit const topKeywords = entries.slice(0, limit).map(([word, count]) => ({ keyword: word, count, })); return topKeywords; } module.exports = { cleanAndTokenize, getNgrams, getWordFrequency, getKeywords, }; // File: memory-processing-toolbox/src/utils/writeToCsv.js //-------------------------------------------------------- // src/utils/writeToCsv.js const { Parser } = require('json2csv'); const fs = require('fs').promises; const logger = require('./logger.js'); /** * writeJsonToCsv * Converts JSON data to CSV format and writes to a file. * @param {Array} jsonData - Array of JSON objects. * @param {Array<string>} fields - Fields to include in the CSV. * @param {string} filePath - Path to the output CSV file. */ async function writeJsonToCsv(jsonData, fields, filePath) { try { const parser = new Parser({ fields }); const csv = parser.parse(jsonData); await fs.writeFile(filePath, csv, 'utf-8'); logger.info(`CSV file written successfully: ${filePath}`); } catch (error) { logger.error(`Failed to write CSV file at ${filePath}: ${error.message}`); throw error; } } module.exports = { writeJsonToCsv, }; // File: memory-processing-toolbox/src/utils/globalVariables.js //------------------------------------------------------------- // src/utils/globalVariables.js // Example: plugin rename mappings const pluginRenames = [ { pluginId: "pluginId_string_here", name: "Automatic Dictionary", }, { pluginId: "pluginId_string_here2", name: "Connect the Dots", }, // ...add more as needed ]; // Example: emoji list with associated categories const categoryEmoji = { Romance: "🌹", Pets: "🐾", Star: "⭐", // ... etc. }; // Example: starred memory IDs const starredMemories = [ // e.g., "be32019e-36c9-4b5f-8043-092cc2aaf5ce" ]; module.exports = { pluginRenames, categoryEmoji, starredMemories, }; // File: memory-processing-toolbox/src/utils/formatMarkdown.js //------------------------------------------------------------ // src/utils/formatMarkdown.js /** * formatCategoriesMarkdown * Formats category summaries into Markdown tables. * @param {Array} categories - Array of { category, count, emoji } * @returns {string} - Markdown formatted string */ function formatCategoriesMarkdown(categories = []) { let markdown = '## Categories Summary\n\n'; markdown += '| Category | Count | Emoji |\n'; markdown += '|----------|-------|-------|\n'; categories.forEach(cat => { markdown += `| ${cat.category} | ${cat.count} | ${cat.emoji} |\n`; }); markdown += '\n'; return markdown; } /** * formatTranscriptMarkdown * Formats transcript segments into Markdown. * @param {Array} transcripts - Array of { memoryId, transcript: [ { text, speaker, ... } ] } * @returns {string} - Markdown formatted string */ function formatTranscriptMarkdown(transcripts = []) { let markdown = '## Transcripts\n\n'; transcripts.forEach(mem => { markdown += `### Memory ID: ${mem.memoryId}\n\n`; markdown += '| Speaker | Text | Start (s) | End (s) |\n'; markdown += '|---------|------|-----------|---------|\n'; mem.transcript.forEach(seg => { markdown += `| ${seg.speaker} | ${seg.text} | ${seg.start} | ${seg.end} |\n`; }); markdown += '\n'; }); return markdown; } // Add more formatting functions as needed module.exports = { formatCategoriesMarkdown, formatTranscriptMarkdown, }; // File: memory-processing-toolbox/src/utils/verifyValidData.js //------------------------------------------------------------- // src/utils/verifyValidData.js const Ajv = require('ajv'); const ajv = new Ajv({ allErrors: true }); const logger = require('./logger.js'); const memoriesSchema = require('../data/memoriesSchema.json'); /** * validateMemories - validates an array of conversation memories against our JSON schema. * @param {Array} memories - The array of memory objects to validate. * @returns {Object} validationResults * - validMemories: An array of valid memory objects * - invalidMemories: An array of objects { index, errors, data } */ function validateMemories(memories = []) { // Compile the schema once const validate = ajv.compile(memoriesSchema); const validMemories = []; const invalidMemories = []; // Validate each memory entry memories.forEach((memory, index) => { const isValid = validate(memory); if (!isValid) { invalidMemories.push({ index, errors: validate.errors, data: memory, }); logger.warn(`Memory at index ${index} failed validation.`); } else { validMemories.push(memory); } }); logger.info(`Validation complete: ${validMemories.length} valid, ${invalidMemories.length} invalid.`); return { validMemories, invalidMemories }; } module.exports = { validateMemories, }; // File: memory-processing-toolbox/src/utils/logger.js //---------------------------------------------------- // src/utils/logger.js const fs = require('fs'); const path = require('path'); const chalk = require('chalk'); const { LOG_DIR } = require('../config.js'); // Ensure logs directory exists if (!fs.existsSync(LOG_DIR)) { fs.mkdirSync(LOG_DIR, { recursive: true }); } // File paths for the log files const infoLogPath = path.join(LOG_DIR, 'info.log'); const errorLogPath = path.join(LOG_DIR, 'error.log'); const debugLogPath = path.join(LOG_DIR, 'debug.log'); // Optional /** * Writes a log message to a specified file. * Adds a timestamp to each entry. */ function writeToFile(filePath, message) { const timestamp = new Date().toISOString(); const entry = `[${timestamp}] ${message} `; fs.appendFile(filePath, entry, 'utf-8', (err) => { if (err) { console.error(chalk.red('Failed to write to log file:'), err); } }); } /** * logger.info * - Writes informational messages to the console in green color. * - Appends the message to info.log with a timestamp. */ function info(...args) { const message = args.join(' '); console.log(chalk.green('[INFO]'), message); writeToFile(infoLogPath, `[INFO] ${message}`); } /** * logger.error * - Writes error messages to the console in red color. * - Appends the message to error.log with a timestamp. */ function error(...args) { const message = args.join(' '); console.error(chalk.red('[ERROR]'), message); writeToFile(errorLogPath, `[ERROR] ${message}`); } /** * logger.warn * - Writes warning messages to the console in yellow color. * - Appends the message to info.log (or a separate warnings file if desired). */ function warn(...args) { const message = args.join(' '); console.warn(chalk.yellow('[WARN]'), message); writeToFile(infoLogPath, `[WARN] ${message}`); } /** * logger.debug * - Writes debug messages to the console in blue color. * - Appends the message to debug.log with a timestamp. * - Only active if DEBUG environment variable is set. */ function debug(...args) { if (process.env.DEBUG) { const message = args.join(' '); console.log(chalk.blue('[DEBUG]'), message); writeToFile(debugLogPath, `[DEBUG] ${message}`); } } module.exports = { info, error, warn, debug, }; // File: memory-processing-toolbox/src/modules/textAnalysis.js //------------------------------------------------------------ // src/modules/textAnalysis.js const logger = require('../utils/logger.js'); const _ = require('lodash'); // A simple default stopwords list (English example). // You can expand this or move it to globalVariables.js if you prefer. const defaultStopwords = [ 'the', 'and', 'of', 'to', 'a', 'i', 'it', 'in', 'is', 'you', 'that', 'for', 'on', 'was', 'with', 'as', 'this', 'but', 'be', 'are', 'have', 'not', 'at', 'or', 'he', 'by', 'from' ]; /** * cleanAndTokenize * - Lowercases text, removes punctuation (basic approach), splits into tokens. * @param {string} text * @returns {Array<string>} tokens */ function cleanAndTokenize(text) { // Convert to lowercase let cleaned = text.toLowerCase(); // Remove punctuation (basic regex approach) // This will remove everything that isn't a letter, number, or space. cleaned = cleaned.replace(/[^a-z0-9s]+/gi, ''); // Split on whitespace const tokens = cleaned.split(/s+/).filter(Boolean); return tokens; } /** * getNgrams * - Converts an array of tokens into an array of n-grams. * e.g., tokens = ["i", "love", "red", "apples"], n=2 => ["i love", "love red", "red apples"] * @param {Array<string>} tokens * @param {number} n - e.g., 2 for bigrams, 3 for trigrams * @returns {Array<string>} */ function getNgrams(tokens, n) { const ngrams = []; for (let i = 0; i <= tokens.length - n; i++) { ngrams.push(tokens.slice(i, i + n).join(' ')); } return ngrams; } /** * getWordFrequency * - Calculates the frequency of each token or n-gram in the transcripts. * @param {Array<object>} transcripts - Array of { text, speaker, ... } * @param {object} options * - n (number): if > 1, calculates n-gram frequencies instead of single-word * - stopwords (Array<string>): custom array of words to ignore * - groupBySpeaker (boolean): if true, returns frequency per speaker * @returns {object} If groupBySpeaker=false => { "word": count, "word2": count, ... } * If groupBySpeaker=true => { speakerA: { "word": count, ... }, speakerB: {...}, ... } */ function getWordFrequency(transcripts = [], options = {}) { logger.info('Calculating word/n-gram frequency...'); const { n = 1, stopwords = defaultStopwords, groupBySpeaker = false, } = options; if (!groupBySpeaker) { // Single frequency map for the entire set of transcripts const freqMap = {}; transcripts.forEach(seg => { const tokens = cleanAndTokenize(seg.text); const finalTokens = n > 1 ? getNgrams(tokens, n) : tokens; finalTokens.forEach(token => { // Skip if token is a stopword (for single words only) // If n>1, tokens are phrases like "machine learning" if (n === 1 && stopwords.includes(token)) return; freqMap[token] = (freqMap[token] || 0) + 1; }); }); return freqMap; } else { // Frequency map separated by speaker const freqMapBySpeaker = {}; transcripts.forEach(seg => { const { speaker } = seg; if (!freqMapBySpeaker[speaker]) { freqMapBySpeaker[speaker] = {}; } const tokens = cleanAndTokenize(seg.text); const finalTokens = n > 1 ? getNgrams(tokens, n) : tokens; finalTokens.forEach(token => { if (n === 1 && stopwords.includes(token)) return; freqMapBySpeaker[speaker][token] = (freqMapBySpeaker[speaker][token] || 0) + 1; }); }); return freqMapBySpeaker; } } /** * getKeywords * - Returns the top N keywords/phrases across transcripts, ignoring stopwords. * This is essentially a "most frequent words" approach, filtered to exclude stopwords. * @param {object} freqMap - output from getWordFrequency (if groupBySpeaker=false) * @param {number} limit - how many top keywords to return * @returns {Array<object>} e.g. [ { keyword: "apple", count: 10 }, ... ] */ function getKeywords(freqMap, limit = 10) { logger.info(`Extracting top ${limit} keywords...`); // freqMap is { "word": count, "anotherWord": count, ... } const entries = Object.entries(freqMap); // Sort by count descending entries.sort((a, b) => b[1] - a[1]); // Return the top limit const topKeywords = entries.slice(0, limit).map(([word, count]) => ({ keyword: word, count, })); return topKeywords; } module.exports = { cleanAndTokenize, getNgrams, getWordFrequency, getKeywords, }; // File: memory-processing-toolbox/src/modules/extractTranscriptSegments.js //------------------------------------------------------------------------- // src/modules/extractTranscriptSegments.js const logger = require('../utils/logger.js'); /** * extractTranscriptSegments * Iterates through each memory and returns an array of relevant transcript info. * * @param {Array} memories - Array of memory objects. * @returns {Array} Array of objects { memoryId, transcript: [ { text, speaker, ... } ] } */ function extractTranscriptSegments(memories = []) { logger.info('Extracting transcript segments...'); const result = memories.map(mem => { const segments = mem.transcript_segments || []; return { memoryId: mem.id, transcript: segments.map(seg => ({ text: seg.text, speaker: seg.speaker, speaker_id: seg.speaker_id, is_user: seg.is_user, start: seg.start, end: seg.end, })), }; }); logger.info(`Transcript segments extracted for ${result.length} memories.`); return result; } module.exports = { extractTranscriptSegments, }; // File: memory-processing-toolbox/src/modules/filterNonDiscarded.js //------------------------------------------------------------------ // src/modules/filterNonDiscarded.js const logger = require('../utils/logger.js'); /** * filterNonDiscarded * Filters out conversations that are discarded or deleted. * * @param {Array} memories - An array of conversation memory objects. * @returns {Array} A new array containing only non-discarded, non-deleted items. */ function filterNonDiscarded(memories = []) { logger.info('Filtering out discarded/deleted conversations...'); const filtered = memories.filter(mem => !mem.discarded && !mem.deleted); logger.info(`Total: ${memories.length}; Non-discarded/deleted: ${filtered.length}`); return filtered; } module.exports = { filterNonDiscarded, }; // File: memory-processing-toolbox/src/modules/extractCategories.js //----------------------------------------------------------------- // src/modules/extractCategories.js const logger = require('../utils/logger.js'); const { categoryEmoji } = require('../utils/globalVariables.js'); /** * extractCategories * Takes an array of memory objects and returns a summary of categories * (plus optional associated emojis, if found in categoryEmoji). * * @param {Array} memories - The array of conversation memory objects. * @returns {Array} categoriesSummary - An array of objects { category, count, emoji } */ function extractCategories(memories = []) { logger.info('Extracting categories from memories...'); // Create a map to track categories and their counts const categoryCounts = {}; memories.forEach(mem => { const category = mem?.structured?.category || 'Uncategorized'; if (!categoryCounts[category]) { categoryCounts[category] = 0; } categoryCounts[category]++; }); // Convert categoryCounts object into an array of { category, count, emoji } const categoriesSummary = Object.entries(categoryCounts).map(([cat, count]) => { return { category: cat, count, emoji: categoryEmoji[cat] || '❓', // fallback emoji if category not listed }; }); logger.info(`Found ${categoriesSummary.length} unique categories.`); return categoriesSummary; } module.exports = { extractCategories, }; // File: memory-processing-toolbox/src/modules/extractPluginResponses.js //---------------------------------------------------------------------- // src/modules/extractPluginResponses.js const logger = require('../utils/logger.js'); const { pluginRenames } = require('../utils/globalVariables.js'); /** * extractPluginResponses * Iterates over each memory, collects plugin results, * and applies any custom renaming/formatting logic. * * @param {Array} memories - Array of memory objects * @returns {Array} Array of objects describing plugin responses for each memory: * [{ * memoryId: <string>, * plugins: [ * { pluginId, displayName, content } * ] * }] */ function extractPluginResponses(memories = []) { logger.info('Extracting plugin responses...'); // Create a helper map: { [pluginId]: displayName } const renameMap = pluginRenames.reduce((acc, item) => { acc[item.pluginId] = item.name; return acc; }, {}); const result = memories.map(mem => { const plugins = mem.plugins_results || []; // Transform each plugin entry const transformedPlugins = plugins.map(plugin => { const displayName = renameMap[plugin.pluginId] || plugin.pluginId; return { pluginId: plugin.pluginId, displayName, content: plugin.content, }; }); return { memoryId: mem.id, plugins: transformedPlugins, }; }); logger.info(`Plugin responses extracted for ${result.length} memories.`); return result; } module.exports = { extractPluginResponses, }; // File: memory-processing-toolbox/.env //------------------------------------- # .env WEBHOOK_SECRET=your_super_secret_key WEBHOOK_PORT=3000 DEBUG=true